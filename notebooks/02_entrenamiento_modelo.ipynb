{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† 02_entrenamiento_modelo.ipynb\n",
    "\n",
    "Este notebook entrena un modelo de aprendizaje profundo (Deep Learning) para reconocer letras del lenguaje de se√±as usando los *landmarks* generados por MediaPipe.\n",
    "\n",
    "## üîÑ Etapas del notebook:\n",
    "1. Importar librer√≠as\n",
    "2. Cargar los datos limpios\n",
    "3. Separar caracter√≠sticas y etiquetas\n",
    "4. Codificar las etiquetas\n",
    "5. Dividir el dataset en entrenamiento y prueba\n",
    "6. Normalizar los datos\n",
    "7. Crear el modelo neuronal\n",
    "8. Compilar el modelo\n",
    "9. Entrenar el modelo\n",
    "10. Visualizar resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 1Ô∏è‚É£ Importar librer√≠as\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el manejo de datos, construcci√≥n del modelo y visualizaci√≥n de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ 2Ô∏è‚É£ Cargar los datos limpios\n",
    "\n",
    "Cargamos el dataset que fue previamente limpiado en el notebook `01_limpieza_datos.ipynb`. Este archivo contiene los landmarks de las manos normalizados y una columna con la etiqueta (letra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/content/dataset_cleaned.csv\")  # Cambia la ruta si est√° en otra ubicaci√≥n\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 3Ô∏è‚É£ Separar caracter√≠sticas (X) y etiquetas (y)\n",
    "\n",
    "Separamos los datos en:\n",
    "- **X:** las coordenadas de los 21 puntos de la mano (x, y, z)\n",
    "- **y:** la letra correspondiente a cada muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('label', axis=1)\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ 4Ô∏è‚É£ Codificar las etiquetas\n",
    "\n",
    "Convertimos las letras (A, B, C...) en n√∫meros para que el modelo pueda entenderlas, y luego usamos *one-hot encoding* para representar cada letra como un vector binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "print(\"Etiquetas codificadas:\", np.unique(y_encoded))\n",
    "print(\"Forma de y_categorical:\", y_categorical.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© 5Ô∏è‚É£ Dividir el dataset en entrenamiento y prueba\n",
    "\n",
    "Reservamos un 80% de los datos para entrenamiento y un 20% para validaci√≥n. Esto nos permite evaluar el desempe√±o del modelo con datos que nunca ha visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Tama√±o de entrenamiento:\", X_train.shape)\n",
    "print(\"Tama√±o de prueba:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 6Ô∏è‚É£ Normalizar los datos\n",
    "\n",
    "Los valores de los landmarks pueden variar mucho dependiendo de la imagen, por eso los normalizamos usando `StandardScaler` para que todos est√©n en una escala similar. Esto mejora el aprendizaje del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 7Ô∏è‚É£ Crear el modelo neuronal\n",
    "\n",
    "Dise√±amos una red neuronal totalmente conectada (Feedforward Neural Network):\n",
    "- Capa 1: 128 neuronas con activaci√≥n ReLU.\n",
    "- Dropout: 30% (reduce sobreajuste).\n",
    "- Capa 2: 64 neuronas con activaci√≥n ReLU.\n",
    "- Dropout: 30%.\n",
    "- Capa de salida: tantas neuronas como letras, con activaci√≥n *softmax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 8Ô∏è‚É£ Compilar el modelo\n",
    "\n",
    "Seleccionamos el optimizador, la funci√≥n de p√©rdida y la m√©trica de evaluaci√≥n:\n",
    "- **Adam:** un optimizador r√°pido y eficiente.\n",
    "- **Categorical Crossentropy:** adecuada para clasificaci√≥n multiclase.\n",
    "- **Accuracy:** mide el porcentaje de predicciones correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 9Ô∏è‚É£ Entrenar el modelo\n",
    "\n",
    "Entrenamos la red neuronal durante 50 √©pocas con un tama√±o de lote (*batch size*) de 32. Se mostrar√° c√≥mo evoluciona la p√©rdida y la precisi√≥n en cada iteraci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä üîü Visualizar resultados\n",
    "\n",
    "Graficamos la precisi√≥n (*accuracy*) de entrenamiento y validaci√≥n para ver si el modelo est√° aprendiendo correctamente o si se est√° sobreajustando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "plt.plot(history.history['val_accuracy'], label='Validaci√≥n')\n",
    "plt.title('Precisi√≥n del modelo')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('Precisi√≥n')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 1Ô∏è‚É£1Ô∏è‚É£ Guardar el modelo entrenado\n",
    "\n",
    "Guardamos el modelo y el codificador de etiquetas para poder usarlos m√°s adelante en la predicci√≥n en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/content/modelo_senas.h5\")\n",
    "import joblib\n",
    "joblib.dump(le, \"/content/label_encoder.pkl\")\n",
    "print(\"Modelo y codificador guardados correctamente ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusi√≥n\n",
    "\n",
    "En este notebook entrenamos un modelo neuronal para reconocer letras del lenguaje de se√±as basado en los landmarks 3D de MediaPipe. \n",
    "\n",
    "El siguiente paso ser√° crear un notebook para la **predicci√≥n en tiempo real**, usando la c√°mara y el modelo `modelo_senas.h5`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_entrenamiento_modelo.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
